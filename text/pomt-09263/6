The evidence with the title 'Health insurance in the United States - Wikipedia ' says Health insurance in the United States is any program that helps pay for medical expenses , whether through privately purchased insurance, social insurance, or a social welfare program funded by the government .  ... Their needs as of 2007 . The share of Americans without health insurance has been cut in half since 2013 . 
